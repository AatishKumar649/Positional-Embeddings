{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBVbVkLzwDKuHkJu07MmpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AatishKumar649/Positional-Embeddings/blob/main/Positional_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
      ],
      "metadata": {
        "id": "dOPywT4DFDQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we focused on very small embedding sizes in this chapter for illustration purposes.\n",
        "\n",
        "We now consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation.\n",
        "\n",
        "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation.\n",
        "\n",
        "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257:"
      ],
      "metadata": {
        "id": "F53540hXFIgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "SkLl1ttOFSO0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GuEwwLq-Eedc"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
        "\n",
        "Let's instantiate the data loader ( Data sampling with a sliding window), first:"
      ],
      "metadata": {
        "id": "WKbtGz5zFZ4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def create_dataloader_v1(raw_text, batch_size, max_length, stride, shuffle):\n",
        "    \"\"\"\n",
        "    Creates a data loader for the given raw text.\n",
        "    This is a placeholder function. You need to implement\n",
        "    the logic for creating your data loader.\n",
        "    \"\"\"\n",
        "    # TODO: Implement your data loader logic here\n",
        "    # For demonstration purposes, this will return sample data\n",
        "    inputs = torch.randint(0, 50257, (batch_size, max_length))\n",
        "    targets = torch.randint(0, 50257, (batch_size, max_length))\n",
        "    # Yielding the inputs and targets to create an iterator\n",
        "    yield inputs, targets\n",
        "\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    \"sample text\", batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)  # Assuming dataloader is iterable\n",
        "inputs, targets = next(data_iter)\n",
        "print(inputs, targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At5eSF1zFtpm",
        "outputId": "ccd4ecfd-b6e3-4aa7-e212-9421367e694e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24563, 29632, 29552,  4034],\n",
            "        [33710, 19062, 39179,  8046],\n",
            "        [34677, 31519, 14879, 25354],\n",
            "        [16692, 24895, 39011, 37020],\n",
            "        [ 2768, 47948, 46043, 41449],\n",
            "        [34554, 14280, 46822, 43036],\n",
            "        [22652, 16325, 25595, 21150],\n",
            "        [ 9183, 28725, 46603, 44440]]) tensor([[23612,  7600, 45678, 34110],\n",
            "        [14648, 12494,  5021, 45063],\n",
            "        [10232, 37834, 46608, 11488],\n",
            "        [32077, 39847, 33408,  2715],\n",
            "        [33998, 20342, 16349,  6792],\n",
            "        [11609, 15722,   988, 48385],\n",
            "        [33751,  9847, 49856, 37880],\n",
            "        [40308, 36629,  1837, 25360]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDnU1k6ZFeAP",
        "outputId": "dbdadb78-885f-41fd-cb8f-cd4ff97c1145"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[24563, 29632, 29552,  4034],\n",
            "        [33710, 19062, 39179,  8046],\n",
            "        [34677, 31519, 14879, 25354],\n",
            "        [16692, 24895, 39011, 37020],\n",
            "        [ 2768, 47948, 46043, 41449],\n",
            "        [34554, 14280, 46822, 43036],\n",
            "        [22652, 16325, 25595, 21150],\n",
            "        [ 9183, 28725, 46603, 44440]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch consists of 8 text samples with 4 tokens each.\n",
        "\n",
        "Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:"
      ],
      "metadata": {
        "id": "FbnLpzwnF3Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNSzG79gF0ew",
        "outputId": "0c6bd1ab-8408-4d77-f2d9-dd71d590fa5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now embedded as a 256-dimensional vector.\n",
        "\n",
        "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same dimension as the token_embedding_layer:"
      ],
      "metadata": {
        "id": "QpKRvnqZF86Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "UuxOkYwEF6Q1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpjanFlRGAWS",
        "outputId": "74ea39e1-551d-4e0b-a574-881a42cd2546"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the preceding code example, the input to the pos_embeddings is usually a placeholder vector torch.arange(context_length), which contains a sequence of numbers 0, 1, ..., up to the maximum input length âˆ’ 1.\n",
        "\n",
        "The context_length is a variable that represents the supported input size of the LLM.\n",
        "\n",
        "Here, we choose it similar to the maximum length of the input text.\n",
        "\n",
        "In practice, input text can be longer than the supported context length, in which case we have to truncate the text.\n",
        "\n",
        "As we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4x256- dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in each of the 8 batches:"
      ],
      "metadata": {
        "id": "i4P9rGxPGEhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVWbg8Q3GCGj",
        "outputId": "988e59c5-006e-4211-9f02-1d67771e18c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input_embeddings we created are the embedded input examples that can now be processed by the main LLM modules"
      ],
      "metadata": {
        "id": "mDIe-_NKGJyF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPeI2y2nGHgO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}